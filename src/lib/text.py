import re
from collections import Counter

def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    for ch in ['\n', '\r', '\t']:
        text = text.replace(ch, ' ')
    if yo2e:
        text = text.replace('Ñ‘', 'Ðµ').replace('Ð', 'Ð•')
    if casefold:
        text = text.casefold()
    text = ' '.join(text.split())
    return text

# print(normalize("ÐŸÑ€Ð˜Ð²Ð•Ñ‚\nÐœÐ˜Ñ€\t"))
# print(normalize("Ñ‘Ð¶Ð¸Ðº, ÐÐ»ÐºÐ°"))
# print(normalize("Hello\r\nWorld")) 
# print(normalize("  Ð´Ð²Ð¾Ð¹Ð½Ñ‹Ðµ   Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹  "))  


def tokenize(text: str) -> list[str]:
    return re.findall(r'\b[\w]+(?:-[\w]+)*\b', text)

# print(tokenize("Ð¿Ñ€Ð¸Ð²ÐµÑ‚ Ð¼Ð¸Ñ€"))
# print(tokenize("hello,world!!!"))
# print(tokenize("Ð¿Ð¾-Ð½Ð°ÑÑ‚Ð¾ÑÑ‰ÐµÐ¼Ñƒ ÐºÑ€ÑƒÑ‚Ð¾"))
# print(tokenize("2025 Ð³Ð¾Ð´"))
# print(tokenize("emoji ðŸ˜€ Ð½Ðµ ÑÐ»Ð¾Ð²Ð¾"))


def count_freq(tokens: list[str]) -> dict[str, int]:
    return dict(Counter(tokens))
def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    return sorted(freq.items(), key=lambda x: (-x[1], x[0]))[:n]

# tokens = ["a", "b", "a", "c", "b", "a"]
# print(count_freq(tokens))
# print(top_n(count_freq(tokens), 2))
